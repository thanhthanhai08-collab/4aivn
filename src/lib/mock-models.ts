
import type { AIModel } from '@/lib/types';

export const mockAIModels: AIModel[] = [
  {
    id: 'gpt-oss-120b-high',
    name: 'GPT-oss-120B (high)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: 'GPT-oss-120B (high) là phiên bản hiệu suất cao của mô hình mã nguồn mở từ OpenAI, với 120 tỷ tham số. Nó được tối ưu hóa cho các tác vụ đòi hỏi sự chính xác cao và khả năng suy luận mạnh mẽ, trong khi vẫn duy trì các lợi ích của việc là một mô hình "open-weight".',
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    contextLengthToken: '131k',
    intelligenceScore: 61,
    pricePerMillionTokens: 0.26,
    speedTokensPerSecond: 223.7,
    latencyFirstChunkSeconds: 0.41,
  },
  {
    id: 'gemini-2.5-pro',
    name: 'Gemini 2.5 Pro',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Google',
    description: `+ Gemini 2.5 Pro (cập nhật mới nhất tháng 06/2025) là mô hình đa phương thức (multimodal) mạnh mẽ nhất trong dòng Gemini do Google phát triển.
+ Gemini 2.5 Pro có thể hiểu và tạo ra nội dung dữ liệu dựa trên nhiều đầu vào khác nhau như văn bản, hình ảnh, video, âm thanh.
+ Mô hình hỗ trợ tới 1 triệu token trong một lần nhập liệu (tương đương khoảng 1,5 triệu từ hoặc 5.000 trang văn bản), với kế hoạch mở rộng lên 2 triệu token trong tương lai gần.Nhờ vậy, Gemini 2.5 Pro có thể xử lý các tài liệu dài, mã nguồn lớn (hơn 50.000 dòng code), video dài gần 1 giờ hoặc âm thanh lên đến 19 giờ trong một lần tương tác duy nhất.
+ Bạn có thể dùng thử Gemini 2.5 Pro ở trên Google AI Studio, Vertex AI, Gemini API.Ngoài ra, Gemini 2.5 Pro cũng được tích hợp trong ứng dụng Gemini dành cho người dùng cao cấp, với các tính năng mới như tăng gấp đôi giới hạn truy vấn, hỗ trợ camera trực tiếp và chia sẻ màn hình trong tính năng Stream.`,
    logoUrl: '/image/Logo Gemini cho bảng xếp hạng.png',
    link: 'https://deepmind.google/technologies/gemini/#introduction',
    userRating: 4.9,
    ratingCount: 250,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý 1 lần là 1 triệu token→ Tức là có thể xử lý khoảng 1,5 triệu từ hoặc 5.000 trang văn bản trong 1 lần xử lý .",
      "Chỉ số thông minh 70 →Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 86%",
      "• GPQA Diamond (Scientific Reasoning) 84%",
      "• Humanity's Exam (Reasoning & Knowledge) 21.1%",
      "• LiveCodeBench (Coding) 80%",
      "• SciCode (Coding) 43%",
      "• HumanEval (Coding)",
      "• MATH-500 (quantitative reasoning) 97%",
      "• AIME 2024 (Competition Math) 89%",
      "Giá trung bình 3.44 USD / 1 triệu token → Dựa trên các thông số giá",
      "• Giá đầu vào 1.25 USD / 1 triệu token.",
      "• Giá đầu ra 10 USD / 1 triệu token.",
      "Tốc độ sinh token khá nhanh là 147.3 token /s.",
      "Độ trễ 38.01s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '1m',
    intelligenceScore: 65,
    pricePerMillionTokens: 3.44,
    speedTokensPerSecond: 147.9,
    latencyFirstChunkSeconds: 37.29,
  },
  {
    id: 'grok-4',
    name: 'Grok 4',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'xAI',
    description: `+ Grok 4 là mô hình AI ngôn ngữ lớn (Large Language Model - LLM) mới nhất do công ty xAI của tỷ phú Elon Musk phát triển, được ra mắt vào tháng 7 năm 2025. Grok 4 đang nhận được rất nhiều quan tâm trong lĩnh vực AI với sự thông minh và các bài benchmark của nó vượt qua Open AI-04, Claude 4 và Gemini 2.5 pro.
+ Grok 4 đã sử dụng hệ thống siêu máy tính gồm hơn 200.000 GPU Nvidia H100 để huấn luyện (gấp đôi so với Grok 3) tất nhiên Grok 4 vẫn giữ được các ưu điểm của Grok 3 với tính năng DeepSearch cho phép truy xuất dữ liệu thời gian thực từ internet và mạng xã hội X, giúp nó cập nhật thông tin nhanh chóng và bắt kịp xu hướng mạng xã hội và vẫn tiếp tục hỗ trợ đa phương thức hình ảnh và văn bản tuy nhiên Grok 4 vẫn chưa hỗ trợ video và giọng nói.
+ Grok 4 tất nhiên vẫn hỗ trợ năng lực suy luận với Grok 4 Thinking và Grok 4 Heavy.
+ Một số người dùng và chuyên gia cho rằng Grok 4 có thể được thiết kế để phản ánh quan điểm chính trị cá nhân của Elon Musk khi trả lời các câu hỏi nhạy cảm, do mô hình tham khảo các bài đăng trên tài khoản X của Musk.`,
    logoUrl: '/image/Logo Grok cho bảng xếp hạng.png',
    link: '#',
    userRating: 4.8,
    ratingCount: 190,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý 1 lần là 256 nghìn token→ Tức là có thể xử lý khoảng 195 nghìn từ hoặc 800 trang văn bản trong 1 lần xử lý.",
      "Chỉ số thông minh 73 →Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 87%",
      "• GPQA Diamond (Scientific Reasoning) 88%",
      "• Humanity's Exam (Reasoning & Knowledge) 23.9%",
      "• LiveCodeBench (Coding) 82%",
      "• SciCode (Coding) 46%",
      "• IFBench (Instruction Following) 54%",
      "• AIME 2025 (Competition Math) 93%",
      "Giá trung bình 6 USD / 1 triệu token → Dựa trên các thông số giá",
      "• Giá đầu vào 3 USD / 1 triệu token.",
      "• Giá đầu ra 15 USD / 1 triệu token.",
      "Tốc độ sinh token là 62.3 token /s.",
      "Độ trễ 10.26s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '256k',
    intelligenceScore: 68,
    pricePerMillionTokens: 6,
    speedTokensPerSecond: 85.2,
    latencyFirstChunkSeconds: 4.87,
  },
  {
    id: 'grok-3-mini-reasoning-high',
    name: 'Grok 3 mini Reasoning (high)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'xAI',
    description: `+Grok 3 Mini là mô hình ngôn ngữ mini do xAI phát triển, tích hợp sâu với nền tảng X (Twitter). Phiên bản "Mini" của Grok 3 được tối ưu để giữ được khả năng tuyệt vời khi bật chế độ suy luận cao (reasoning (high)), trong khi vẫn đảm bảo tốc độ phản hồi nhanh và khả năng triển khai nhẹ hơn trên nhiều nền tảng.
+Triển khai linh hoạt: Nhờ dung lượng nhỏ hơn, Grok 3 Mini dễ dàng tích hợp trên nhiều nền tảng, từ web, di động đến các ứng dụng doanh nghiệp.
+ Grok 3 mini do có sự hỗ trợ từ mạng xã hội X (Twitter) từ đó có khả năng truy cập và cập nhật dữ liệu thời gian thực từ X, giúp cung cấp thông tin mới nhất và phù hợp với các sự kiện đang diễn ra .
+ Phong cách phản hồi tự nhiên, thân thiện được học từ những người dùng mạng xã hội X nên Grok 3 và Grok 3 mini được đánh giá rất cao với cách trả lời gần gũi, hài hước và dễ tiếp cận với người dùng Việt Nam.`,
    logoUrl: '/image/Logo Grok cho bảng xếp hạng.png',
    link: '#',
    userRating: 4.8,
    ratingCount: 180,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý 1 lần là 1 triệu token.",
      "• Tức là có thể xử lý khoảng 1,5 triệu từ hoặc 5.000 trang văn bản trong 1 lần xử lý.",
      "Chỉ số thông minh 67",
      "• Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 83%",
      "• GPQA Diamond (Scientific Reasoning) 79%",
      "• Humanity's Exam (Reasoning & Knowledge) 11.1%",
      "• LiveCodeBench (Coding) 70%",
      "• SciCode (Coding) 41%",
      "• HumanEval (Coding) 98%",
      "• MATH-500 (Quantitative reasoning) 99%",
      "• AIME 2024 (Competition Math) 93%",
      "Giá trung bình 0.35 USD / 1 triệu token",
      "• Giá đầu vào 0.3 USD / 1 triệu token.",
      "• Giá đầu ra 0.5 USD / 1 triệu token.",
      "Tốc độ sinh token khá nhanh là 209.2 token /s.",
      "Độ trễ 0.32s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '1m',
    intelligenceScore: 58,
    pricePerMillionTokens: 0.35,
    speedTokensPerSecond: 207.9,
    latencyFirstChunkSeconds: 0.63,
  },
  {
    id: 'openai-o3-pro',
    name: 'Open AI o3-pro',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: `+ OpenAI o3 Pro là phiên bản nâng cấp mới nhất thuộc dòng mô hình lý luận o3, chính thức ra mắt vào ngày 10/6/2025. Đây là bước tiến vượt bậc trong công nghệ AI của OpenAI, được tích hợp trong các gói ChatGPT Pro, Team, Enterprise và API, nhằm thay thế hoàn toàn phiên bản o1 Pro trước đó.
+ OpenAI khuyến nghị người dùng lựa chọn o3 Pro cho các tác vụ đòi hỏi độ tin cậy và độ chính xác cao, nơi việc chờ đợi vài phút là xứng đáng.`,
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    userRating: 4.9,
    ratingCount: 320,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 200 nghìn token→ Tức là có thể xử lý khoảng 150 nghìn từ hoặc 600 trang văn bản trong 1 lần xử lý .",
      "Chỉ số thông minh 71 → Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 85%",
      "• GPQA Diamond (Scientific Reasoning) 84%",
      "• Humanity's Exam (Reasoning & Knowledge) 18.2%",
      "• LiveCodeBench (Coding) 81%",
      "• SciCode (Coding) 42%",
      "• HumanEval (Coding) 99%",
      "• MATH-500 (Quantitative reasoning) 99%",
      "• AIME 2024 (Competition Math) 93%",
      "Giá trung bình 35 USD / 1 triệu token → Dựa trên các thông số giá",
      "• Giá đầu vào 20 USD / 1 triệu token.",
      "• Giá đầu ra 80 USD / 1 triệu token.",
      "Tốc độ sinh token là 32.7 token /s.",
      "Độ trễ 78.85s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '200k',
    intelligenceScore: 68,
    pricePerMillionTokens: 35,
    speedTokensPerSecond: 28.1,
    latencyFirstChunkSeconds: 79.62,
  },
   {
    id: 'gemini-2.5-flash-reasoning',
    name: 'Gemini 2.5 Flash (Reasoning)',
    type: 'AI Đa phương thức',
    developer: 'Google',
    description: `+ Gemini 2.5 Flash (Reasoning) cập nhật mới nhất là tháng 6-2025 là phiên bản nâng cấp của Gemini 2.0 Flash đặc biệt với chắc năng suy luận nâng cao (Reasoning) mang đến hiệu suất ở mức chấp nhận được kết hợp với chi phí và độ mượt mà.
+ Nâng cấp từ Gemini 2.0 flash nên độ dài ngữ cảnh (context window) ở mức 1 triệu token là mức trở nên bình thường đối với nhà Google.
+ Gemini 2.5 flash đặc biệt phù hợp với các ứng dụng chatbot hoặc web /app cần phản hồi nhanh mà vẫn giữ được sự thông minh. Vì vậy sẽ là đối thủ trực tiếp với Claude 4 Sonnet Thinking nhưng Claude 4 Sonnet có giá chát hơn hẳn.`,
    logoUrl: '/image/Logo Gemini cho bảng xếp hạng.png',
    link: 'https://deepmind.google/technologies/gemini/',
    userRating: 4.9,
    ratingCount: 195,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 1 triệu token",
      "• Tức là có thể xử lý khoảng 1,5 triệu từ hoặc 5.000 trang văn bản trong 1 lần xử lý.",
      "Chỉ số thông minh 65",
      "• Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 83%",
      "• GPQA Diamond (Scientific Reasoning) 79%",
      "• Humanity's Exam (Reasoning & Knowledge) 11.1%",
      "• LiveCodeBench (Coding) 70%",
      "• SciCode (Coding) 39%",
      "• HumanEval (Coding) 96%",
      "• MATH-500 (Quantitative reasoning) 98%",
      "• AIME 2024 (Competition Math) 82%",
      "Giá trung bình 0.99 USD / 1 triệu token",
      "• Giá đầu vào 1.1 USD / 1 triệu token.",
      "• Giá đầu ra 4.4 USD / 1 triệu token.",
      "Tốc độ sinh token là 351.2 token /s.",
      "Độ trễ 13.24s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '1m',
    intelligenceScore: 58,
    pricePerMillionTokens: 0.85,
    speedTokensPerSecond: 299.9,
    latencyFirstChunkSeconds: 17.0,
  },
  {
    id: 'claude-4-opus-thinking',
    name: 'Claude 4 Opus Thinking',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Anthropic',
    description: `+ Claude 4 Opus Thinking là phiên bản cao cấp nhất trong dòng mô hình Claude 4 của Anthropic ( vừa ra mắt tháng 5 năm 2025) Claude Opus 4 được tối ưu đặc biệt cho các tác vụ đòi hỏi khả năng suy luận sâu với nhiều bước phức tạp đi kèm độ chính xác.
+ Claude 4 Opus Thinking thật sự có lẽ là mô hình thông minh nhất hiện nay vì các lý do
+ Claude 4 Opus có khả năng suy luận qua nhiều bước (multi-step reasoning), giúp mô hình xử lý vấn đề logic và có hệ thống, giảm tối đa"lối tắt" hay thủ thuật không những thế nó còn bổ sung chế độ Thinking cho phép mô hình tạm dừng để cân nhắc kỹ lưỡng nhiều bước trước khi đưa ra câu trả lời cuối cùng, điều đó giúp nó thông minh hơn 65% so với Claude 3.7 Sonnet.
+ Trong điểm thông minh ở đây không bổ sung benchmark như SWE-bench (78.3%) và Terminal-bench (85.2%) mà Claude 4 Opus Thinking đạt được chứ không là nó bỏ xa các mô hình GPT-4.1 (54.6%) và Gemini 2.5 Pro (63.8%).
+ Claude 4 Opus Thinking còn rất thân thiện và khả năng hoạt động ổn định với tiếng Việt. Mô hình này là ít lỗi nhất và ổn định nhất so với GPT 4.1 và Gemini 2.5 Pro nếu ai đã từng trải nghiệm nhưng có nhược điểm là giá rất cao vì vậy chỉ phù hợp với những ai có điều kiện và mô hình là doanh nghiệp.`,
    logoUrl: '/image/Logo Claude AI cho bảng xếp hạng.png',
    link: 'https://www.anthropic.com/news/claude-3-family',
    userRating: 4.9,
    ratingCount: 280,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 200 nghìn token→ Tức là có thể xử lý khoảng 150 nghìn từ hoặc 600 trang văn bản trong 1 lần xử lý .",
        "Chỉ số thông minh 64 →Dựa trên trung bình các điểm benmark là",
        "• MMLU-Pro (Reasoning & Knowledge) 87%",
        "• GPQA Diamond (Scientific Reasoning) 80%",
        "• Humanity's Exam (Reasoning & Knowledge) 11.7%",
        "• LiveCodeBench (Coding) 64%",
        "• SciCode (Coding) 40%",
        "• HumanEval (Coding)",
        "• MATH-500 (Quantitative reasoning) 98%",
        "• AIME 2024 (Competition Math) 76%",
        "Giá trung bình 30.0 USD / 1 triệu token → Dựa trên các thông số giá",
        "• Giá đầu vào 15.0 USD / 1 triệu token.",
        "• Giá đầu ra 75.0 USD / 1 triệu token.",
        "Tốc độ sinh token là 55.5 token /s.",
        "Độ trễ 3.22s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '200k',
    intelligenceScore: 55,
    pricePerMillionTokens: 30.00,
    speedTokensPerSecond: 19.3,
    latencyFirstChunkSeconds: 1.42,
  },
  {
    id: 'qwen3-235b-reasoning',
    name: 'Qwen3 235B 2507 (Reasoning)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Alibaba',
    description: `+ Qwen3-235B-A22B, hay gọi tắt là Qwen 235B, là phiên bản flagship mới nhất trong dòng mô hình Qwen do Alibaba phát triển, ra mắt vào đầu năm 2025. Đây là một trong những mô hình Al tiên tiến nhất hiện nay, nổi bật với khả năng suy luận đa bước sâu sắc, hiệu suất cao trong toán học, lập trình và xử lý ngôn ngữ tự nhiên, đồng thời hỗ trợ đa ngôn ngữ lên đến 119 ngôn ngữ và phương ngữ.
+ Mô hình Qwen 235B hoạt động ở 2 chế độ đặc biệt ở chế độ Thinking (chế độ suy nghĩ): Mô hình thực hiện suy luận từng bước, phù hợp với các bài toán phức tạp như toán học, lập trình, và các tác vụ logic đa bước, còn khi không bật Thinking thì mô hình chỉ đáp ứng những câu hỏi đơn giản yêu cầu truy xuất nhanh.
+ Mô hình sở hữu 235 tỷ tham số tổng thể, trong đó mỗi truy vấn chỉ kích hoạt 22 tỷ tham số, giúp giảm đáng kể chi phí tính toán và độ trễ mà vẫn duy trì hiệu suất .`,
    logoUrl: '/image/Logo Qwen cho bảng xếp hạng.png',
    link: '#',
    userRating: 4.9,
    ratingCount: 150,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 256 nghìn token→ Tức là có thể xử lý khoảng 195 nghìn từ hoặc 800 trang văn bản trong 1 lần xử lý.",
      "Chỉ số thông minh 69 →Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 84%",
      "• GPQA Diamond (Scientific Reasoning) 79%",
      "• Humanity's Exam (Reasoning & Knowledge) 15%",
      "• LiveCodeBench (Coding) 79%",
      "• SciCode (Coding) 42%",
      "• IFBench (Instruction Following) 51%",
      "• AIME 2025 (Competition Math) 91%",
      "Giá trung bình 2.63 USD / 1 triệu token → Dựa trên các thông số giá",
      "• Giá đầu vào 0.7 USD / 1 triệu token.",
      "• Giá đầu ra 8.4 USD / 1 triệu token.",
      "Tốc độ sinh token là 62.9 token /s.",
      "Độ trễ 1.35s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '256k',
    intelligenceScore: 64,
    pricePerMillionTokens: 2.63,
  },
  {
    id: 'gpt-5-high',
    name: 'GPT-5 (high)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: 'GPT-5 (high) là phiên bản hiệu suất cao nhất của dòng mô hình GPT-5, được tối ưu hóa cho các tác vụ đòi hỏi sự chính xác và suy luận phức tạp. Nó cung cấp hiệu suất hàng đầu cho các ứng dụng doanh nghiệp quan trọng.',
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    contextLengthToken: '400k',
    intelligenceScore: 68,
    pricePerMillionTokens: 3.44,
    speedTokensPerSecond: 74.5,
    latencyFirstChunkSeconds: 122.57
  },
  {
    id: 'gpt-5-medium',
    name: 'GPT-5 (medium)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: 'GPT-5 (medium) là phiên bản cân bằng giữa hiệu suất và tốc độ của dòng GPT-5, cung cấp khả năng mạnh mẽ với tốc độ phản hồi nhanh hơn, phù hợp cho nhiều ứng dụng tương tác.',
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    contextLengthToken: '400k',
    intelligenceScore: 68,
    pricePerMillionTokens: 3.44,
    speedTokensPerSecond: 123.2,
    latencyFirstChunkSeconds: 37.50,
  },
  {
    id: 'gpt-5-mini',
    name: 'GPT-5 mini',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: 'GPT-5 mini là phiên bản nhỏ gọn và hiệu quả của dòng GPT-5, được tối ưu hóa cho tốc độ và chi phí, mang lại hiệu suất mạnh mẽ cho các ứng dụng yêu cầu phản hồi nhanh.',
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    contextLengthToken: '400k',
    intelligenceScore: 64,
    pricePerMillionTokens: 0.69,
    speedTokensPerSecond: 104.3,
    latencyFirstChunkSeconds: 23.97,
  },
  {
    id: 'gpt-5-low',
    name: 'GPT-5 (low)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: 'GPT-5 (low) là phiên bản giá rẻ của dòng GPT-5, được thiết kế cho các ứng dụng không yêu cầu hiệu suất cao nhất nhưng cần một giải pháp kinh tế và hiệu quả.',
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    contextLengthToken: '400k',
    intelligenceScore: 63,
    pricePerMillionTokens: 3.44,
    speedTokensPerSecond: 204.9,
    latencyFirstChunkSeconds: 16.86
  },
  {
    id: 'openai-o4-mini-high',
    name: 'Open AI o4-mini (high)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: `+ OpenAI o4 Mini (High) là phiên bản nâng cấp hiệu suất của dòng mô hình o4 Mini, được ra mắt vào đầu năm 2025. Đây là biến thể được tối ưu để xử lý các tác vụ suy luận phức tạp hơn, phù hợp với các nhu cầu nghiên cứu khoa học nâng cao, lập trình mã nguồn phức tạp và giải quyết các bài toán toán học khó khăn, trong khi vẫn giữ được chi phí vận hành thấp và tốc độ phản hồi nhanh hơn nhiều so với các mô hình cao cấp như o3 hay GPT-4.
+ Mặc dù có sức mạnh tính toán gần bằng các mô hình cao cấp, o4 Mini (High) vẫn duy trì chi phí vận hành hợp lý, giúp người dùng tiết kiệm ngân sách so với việc sử dụng các mô hình lớn hơn. Tốc độ phản hồi của mô hình vẫn nhanh, tuy có độ trễ cao hơn một chút so với o4 Mini tiêu chuẩn do yêu cầu tính toán phức tạp hơn.
+ OpenAI o4 Mini (High) phù hợp với các ứng dụng đòi hỏi độ chính xác cao và suy luận chuyên sâu như soạn truy vấn SQL phức tạp, giải thích các khái niệm khoa học nâng cao, phát triển phần mềm phức tạp và nghiên cứu chuyên ngành.`,
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    userRating: 4.9,
    ratingCount: 210,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 200 nghìn token→ Tức là có thể xử lý khoảng 150 nghìn từ hoặc 600 trang văn bản trong 1 lần xử lý .",
      "Chỉ số thông minh 70 →Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 83%",
      "• GPQA Diamond (Scientific Reasoning) 78%",
      "• Humanity's Exam (Reasoning & Knowledge) 17.5%",
      "• LiveCodeBench (Coding) 80%",
      "• SciCode (Coding) 47%",
      "• HumanEval (Coding) 99%",
      "• MATH-500 (Quantitative reasoning) 99%",
      "• AIME 2024 (Competition Math) 94%",
      "Giá trung bình 1.93 USD / 1 triệu token → Dựa trên các thông số giá",
      "• Giá đầu vào 1.1 USD / 1 triệu token.",
      "• Giá đầu ra 4.5 USD / 1 triệu token.",
      "Tốc độ sinh token khá nhanh là 115.5 token /s.",
      "Độ trễ 48.33s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '200k',
    intelligenceScore: 65,
    pricePerMillionTokens: 1.93,
    speedTokensPerSecond: 135.1,
    latencyFirstChunkSeconds: 41.45,
  },
  {
    id: 'qwen3-32b-a3b-reasoning',
    name: 'Qwen3 30B A3B (Reasoning)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Alibaba',
    description: `+ Qwen3 30B A3B là một trong hai phiên bản mô hình Mixture-of-Experts (MoE) tiên tiến thuộc dòng Qwen3 do Alibaba Cloud phát triển, ra mắt đầu năm 2025. Với tổng cộng 30 tỷ tham số nhưng chỉ kích hoạt khoảng 3 tỷ tham số cho mỗi tác vụ, Qwen3 30B A3B tối ưu hóa hiệu quả tính toán trong khi vẫn duy trì khả năng suy luận và xử lý ngôn ngữ vượt trội.
+ Qwen3 30B A3B hỗ trợ hơn 100 ngôn ngữ và phương ngữ, trong đó có tiếng Việt, giúp mô hình dễ dàng ứng dụng trên phạm vi toàn cầu với khả năng giao tiếp tự nhiên và chính xác.
+ Mô hình tích hợp chế độ suy nghĩ (reasoning mode), cho phép thực hiện lập luận từng bước để giải quyết các vấn đề phức tạp như toán học, lập trình và logic đa bước, đồng thời có chế độ không suy nghĩ cho các câu hỏi đơn giản nhằm tối ưu tốc độ phản hồi.
+ Người dùng có thể truy cập miễn phí Qwen3 30B A3B thông qua các API do OpenRouter và Alibaba Cloud cung cấp. Mô hình cũng hỗ trợ tích hợp dễ dàng qua các thư viện phổ biến như Hugging Face Transformers, giúp nhà phát triển nhanh chóng xây dựng và triển khai các ứng dụng Al đa dạng.`,
    logoUrl: '/image/Logo Qwen cho bảng xếp hạng.png',
    link: '#',
    userRating: 4.9,
    ratingCount: 130,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 128 nghìn token→ Tức là có thể xử lý khoảng 120 nghìn từ hoặc 400 trang văn bản trong 1 lần xử lý .",
        "Chỉ số thông minh 56 →Dựa trên trung bình các điểm benmark là",
        "• MMLU-Pro (Reasoning & Knowledge) 78%",
        "• GPQA Diamond (Scientific Reasoning) 62%",
        "• Humanity's Exam (Reasoning & Knowledge) 6.6%",
        "• LiveCodeBench (Coding) 51%",
        "• SciCode (Coding) 28%",
        "• HumanEval (Coding)",
        "• MATH-500 (Quantitative reasoning) 96%",
        "• AIME 2024 (Competition Math) 75%",
        "Giá trung bình 0.75 USD / 1 triệu token → Dựa trên các thông số giá",
        "• Giá đầu vào 0.2 USD / 1 triệu token.",
        "• Giá đầu ra 2.4 USD / 1 triệu token.",
        "Tốc độ sinh token là 91.7 token /s.",
        "Độ trễ 1.05s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '128k',
    intelligenceScore: 56,
    pricePerMillionTokens: 0.75,
    speedTokensPerSecond: 91.7,
    latencyFirstChunkSeconds: 1.05,
  },
  {
    id: 'qwenq-32b',
    name: 'QwenQ-32B',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Alibaba',
    description: `+ Qwen QwQ-32B (viết tắt của "Qwen With Questions") là mô hình ngôn ngữ lớn (LLM) với 32 tỷ tham số do Alibaba phát triển, ra mắt đầu năm 2025.
+ Mặc dù chỉ có 32 tỷ tham số, QwQ-32B được tối ưu để hoạt động hiệu quả với yêu cầu tài nguyên thấp hơn nhiều so với các mô hình lớn hơn như DeepSeek-R1 (671 tỷ tham số). Mô hình chỉ cần khoảng 24GB VRAM để chạy, phù hợp với các GPU phổ biến như Nvidia RTX 4090 hoặc cao hơn, giúp mở rộng khả năng triển khai trên nhiều thiết bị.
+ QwQ-32B được phát hành dưới giấy phép Apache 2.0 (mã nguồn mở), cho phép các nhà phát triển, doanh nghiệp và cộng đồng nghiên cứu sử dụng, tùy chỉnh và tích hợp mô hình một cách tự do.
+ Mô hình vẫn còn một số hạn chế như khả năng trộn lẫn ngôn ngữ hoặc chuyển đổi mã không mong muốn, có thể dẫn đến phản hồi dài dòng hoặc vòng lặp suy luận đệ quy. Ngoài ra, cần áp dụng các biện pháp an toàn để đảm bảo hiệu suất và tránh các kết quả không phù hợp khi triển khai thực tế.`,
    logoUrl: '/image/Logo Qwen cho bảng xếp hạng.png',
    link: '#',
    userRating: 4.9,
    ratingCount: 140,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 131 nghìn token → Tức là có thể xử lý khoảng 135 nghìn từ hoặc 500 trang văn bản trong 1 lần xử lý.",
        "Chỉ số thông minh 58 →Dựa trên trung bình các điểm benmark là",
        "• MMLU-Pro (Reasoning & Knowledge) 80%",
        "• GPQA Diamond (Scientific Reasoning) 67%",
        "• Humanity's Exam (Reasoning & Knowledge) 8.3%",
        "• LiveCodeBench (Coding) 55%",
        "• SciCode (Coding) 35%",
        "• HumanEval (Coding)",
        "• MATH-500 (Quantitative reasoning) 96%",
        "• AIME 2024 (Competition Math) 81%",
        "Giá trung bình 0.47 USD / 1 triệu token → Dựa trên các thông số giá",
        "• Giá đầu vào 0.4 USD / 1 triệu token.",
        "• Giá đầu ra 0.68 USD / 1 triệu token.",
        "Tốc độ sinh token là 99 token /s.",
        "Độ trễ 0.43s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '131k',
    intelligenceScore: 48,
    pricePerMillionTokens: 0.49,
    speedTokensPerSecond: 54.4,
    latencyFirstChunkSeconds: 0.55,
  },
  {
    id: 'claude-4.1-opus',
    name: 'Claude 4.1 Opus',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Anthropic',
    description: `+ Claude 4.1 Opus là phiên bản cao cấp nhất trong dòng mô hình Claude 4 của Anthropic ( vừa ra mắt tháng 5 năm 2025) Claude Opus 4 được tối ưu đặc biệt cho các tác vụ đòi hỏi khả năng suy luận sâu với nhiều bước phức tạp đi kèm độ chính xác.
+ Claude 4.1 Opus thật sự có lẽ là mô hình thông minh nhất hiện nay vì các lý do
+ Claude 4.1 Opus có khả năng suy luận qua nhiều bước (multi-step reasoning), giúp mô hình xử lý vấn đề logic và có hệ thống, giảm tối đa"lối tắt" hay thủ thuật không những thế nó còn bổ sung chế độ  cho phép mô hình tạm dừng để cân nhắc kỹ lưỡng nhiều bước trước khi đưa ra câu trả lời cuối cùng, điều đó giúp nó thông minh hơn 65% so với Claude 3.7 Sonnet.
+ Trong điểm thông minh ở đây không bổ sung benchmark như SWE-bench (78.3%) và Terminal-bench (85.2%) mà Claude 4.1 Opus đạt được chứ không là nó bỏ xa các mô hình GPT-4.1 (54.6%) và Gemini 2.5 Pro (63.8%).
+ Claude 4.1 Opus còn rất thân thiện và khả năng hoạt động ổn định với tiếng Việt. Mô hình này là ít lỗi nhất và ổn định nhất so với GPT 4.1 và Gemini 2.5 Pro nếu ai đã từng trải nghiệm nhưng có nhược điểm là giá rất cao vì vậy chỉ phù hợp với những ai có điều kiện và mô hình là doanh nghiệp.`,
    logoUrl: '/image/Logo Claude AI cho bảng xếp hạng.png',
    link: 'https://www.anthropic.com/news/claude-3-family',
    userRating: 4.5,
    ratingCount: 240,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 200 nghìn token→ Tức là có thể xử lý khoảng 150 nghìn từ hoặc 600 trang văn bản trong 1 lần xử lý .",
        "Chỉ số thông minh 58 →Dựa trên trung bình các điểm benmark là",
        "• MMLU-Pro (Reasoning & Knowledge) 86%",
        "• GPQA Diamond (Scientific Reasoning) 70%",
        "• Humanity's Exam (Reasoning & Knowledge) 5.9%",
        "• LiveCodeBench (Coding) 54%",
        "• SciCode (Coding) 41%",
        "• HumanEval (Coding) 97%",
        "• MATH-500 (Quantitative reasoning) 94%",
        "• AIME 2024 (Competition Math) 56%",
        "Giá trung bình 30 USD / 1 triệu token → Dựa trên các thông số giá",
        "• Giá đầu vào 15.0 USD / 1 triệu token.",
        "• Giá đầu ra 75.0 USD / 1 triệu token.",
        "Tốc độ sinh token là 61.1 token /s.",
        "Độ trễ 2.33s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '200k',
    intelligenceScore: 49,
    pricePerMillionTokens: 30.00,
    speedTokensPerSecond: 36.5,
    latencyFirstChunkSeconds: 1.53,
  },
  {
    id: 'claude-4.1-opus-thinking',
    name: 'Claude 4.1 Opus Thinking',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Anthropic',
    description: `+ Claude 4.1 Opus Thinking ra mắt vào đầu năm 2025, là phiên bản nâng cấp đáng kể so với Claude 4 Opus, được Anthropic phát triển như một mô hình suy luận lai (hybrid reasoning model) hàng đầu trên thị trường.
+ Claude 4.1 Opus Thinking không phải là một mô hình riêng biệt cho suy luận mà là sự kết hợp giữa khả năng phản hồi nhanh và suy luận sâu, tương tự cách con người có thể trả lời ngay hoặc dành thời gian suy nghĩ kỹ càng.
+ Khi bật suy luận (thinking), mô hình Claude 4.1 Opus Thinking sẽ tự phản tỉnh và thực hiện suy luận đa bước trước khi trả lời, giúp nâng cao hiệu quả trong các lĩnh vực như toán học, vật lý, lập trình, và các tác vụ hướng dẫn phức tạp.
+ Mặc dù không phải là mô hình mới nhất, nó vẫn nổi lên như một giải pháp mạnh mẽ, cân bằng giữa hiệu suất và chi phí, phù hợp cho các doanh nghiệp cần khả năng suy luận cao cấp mà không muốn chi trả mức giá cao nhất của các mô hình chuyên dụng.`,
    logoUrl: '/image/Logo Claude AI cho bảng xếp hạng.png',
    link: 'https://www.anthropic.com/news/claude-3-5-sonnet',
    userRating: 4.9,
    ratingCount: 220,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 200 nghìn token → Tức là có thể xử lý khoảng 150 nghìn từ hoặc 600 trang văn bản trong 1 lần xử lý.",
        "Chỉ số thông minh 57 →Dựa trên trung bình các điểm benmark là",
        "• MMLU-Pro (Reasoning & Knowledge) 84%",
        "• GPQA Diamond (Scientific Reasoning) 77%",
        "• Humanity's Exam (Reasoning & Knowledge) 10.3%",
        "• LiveCodeBench (Coding) 47%",
        "• SciCode (Coding) 40%",
        "• HumanEval (Coding) 98%",
        "• MATH-500 (Quantitative reasoning) 95%",
        "• AIME 2024 (Competition Math) 49%",
        "Giá trung bình 6 USD / 1 triệu token → Dựa trên các thông số giá",
        "• Giá đầu vào 3.0 USD / 1 triệu token.",
        "• Giá đầu ra 15.0 USD / 1 triệu token.",
        "Tốc độ sinh token là 87.1 token /s.",
        "Độ trễ 1.32s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '200k',
    intelligenceScore: 61,
    pricePerMillionTokens: 30.00,
    speedTokensPerSecond: 30.5,
    latencyFirstChunkSeconds: 1.62,
  },
  {
    id: 'claude-4-sonnet-thinking',
    name: 'Claude 4 Sonnet Thinking',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Anthropic',
    description: `+ Claude 4 Sonnet là một trong hai phiên bản chủ lực của dòng mô hình Claude 4 do Anthropic ra mắt
vào tháng 5 năm 2025, bên cạnh phiên bản Opus 4. Sonnet 4 được thiết kế để cân bằng giữa hiệu suất
và chi phí, phù hợp với các ứng dụng cần khối lượng xử lý lớn, tốc độ phản hồi nhanh nhưng vẫn giữ
được khả năng suy luận và lập trình nâng cao.
+ Claude 4 Sonnet là mô hình mạnh mẽ nhất của Claude 4 Sonnet với chức năng
cho các nhiệm vụ phức tạp, giúp mô hình linh hoạt xử lý từ các câu hỏi nhanh đến các bài toán logic đa
bước nhưng vẫn giữ được giá rẻ hơn 20% so với Claude 4 Opus
+ Khi được cấp quyền truy cập file, Sonnet 4 có thể lưu trữ và trích xuất thông tin quan trọng để duy trì
ngữ cảnh trong các dự án dài hạn như nghiên cứu hoặc phát triển phần mềm, giúp tăng tính liên tục và
hiệu quả trong công việc.`,
    logoUrl: '/image/Logo Claude AI cho bảng xếp hạng.png',
    link: 'https://www.anthropic.com/news/claude-3-family',
    userRating: 4.5,
    ratingCount: 240,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 200 nghìn token→ Tức là có thể xử lý khoảng 150 nghìn từ hoặc 600 trang văn bản trong 1 lần xử lý .",
        "Chỉ số thông minh 53 →Dựa trên trung bình các điểm benmark là",
        "• MMLU-Pro (Reasoning & Knowledge) 84%",
        "• GPQA Diamond (Scientific Reasoning) 68%",
        "• Humanity's Exam (Reasoning & Knowledge) 4%",
        "• LiveCodeBench (Coding) 45%",
        "• SciCode (Coding) 37%",
        "• HumanEval (Coding) 97%",
        "• MATH-500 (Quantitative reasoning) 93%",
        "• AIME 2024 (Competition Math) 41%",
        "Giá trung bình 6 USD / 1 triệu token → Dựa trên các thông số giá",
        "• Giá đầu vào 3.0 USD / 1 triệu token.",
        "• Giá đầu ra 15.0 USD / 1 triệu token.",
        "Tốc độ sinh token là 48.1 token /s.",
        "Độ trễ 1.52s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '1m',
    intelligenceScore: 59,
    pricePerMillionTokens: 6.00,
    speedTokensPerSecond: 58.2,
    latencyFirstChunkSeconds: 0.95,
  },
  {
    id: 'gpt-5-nano',
    name: 'GPT-5 nano',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: 'GPT-5 nano là phiên bản nhỏ gọn và hiệu quả nhất trong dòng sản phẩm GPT-5, được tối ưu hóa cho tốc độ và chi phí vận hành. Với kiến trúc nhẹ, mô hình này mang lại khả năng phản hồi tức thì với độ trễ cực thấp, phù hợp cho các ứng dụng đòi hỏi xử lý khối lượng lớn và tương tác theo thời gian thực như chatbot hỗ trợ khách hàng, phân tích dữ liệu nhanh và các tác vụ tự động hóa quy mô lớn. Dù nhỏ gọn, GPT-5 nano vẫn duy trì được hiệu suất đáng kể, là một giải pháp kinh tế nhưng mạnh mẽ.',
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    userRating: 4.5,
    ratingCount: 190,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 128 nghìn token→ Tức là có thể xử lý khoảng 120 nghìn từ hoặc 400 trang văn bản trong 1 lần xử lý.",
      "Chỉ số thông minh 53 →Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge)",
      "• GPQA Diamond (Scientific Reasoning) 71%",
      "• Humanity's Exam (Reasoning & Knowledge)",
      "• LiveCodeBench (Coding)",
      "• SciCode (Coding)",
      "• HumanEval (Coding) 89%",
      "• MATH-500 (Quantitative reasoning)",
      "• AIME 2024 (Competition Math) 37%",
      "Giá trung bình 93.75 USD / 1 triệu token → Dựa trên các thông số giá",
      "• Giá đầu vào 75 USD / 1 triệu token.",
      "• Giá đầu ra 150 USD / 1 triệu token.",
      "Tốc độ sinh token là 76.5 token /s.",
      "Độ trễ 0.94s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '400k',
    intelligenceScore: 54,
    pricePerMillionTokens: 0.14,
    speedTokensPerSecond: 213.3,
    latencyFirstChunkSeconds: 35.77,
  },
  {
    id: 'gpt-oss-20b-high',
    name: 'GPT-oss-20B (high)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: 'GPT-oss-20B (high) là phiên bản hiệu suất cao của mô hình mã nguồn mở 20 tỷ tham số từ OpenAI. Nó được tối ưu hóa cho các nhiệm vụ đòi hỏi độ chính xác cao và khả năng suy luận mạnh mẽ, trong khi vẫn duy trì các lợi ích của việc là một mô hình "open-weight", phù hợp cho việc triển khai trên phần cứng tiêu dùng.',
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    userRating: 4.5,
    ratingCount: 225,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 131 nghìn token.",
        "Chỉ số thông minh 57",
        "Giá trung bình 0.16 USD / 1 triệu token.",
        "Tốc độ sinh token là 263.2 token /s.",
        "Độ trễ 0.35s."
    ],
    isFavorite: false,
    contextLengthToken: '131k',
    intelligenceScore: 49,
    pricePerMillionTokens: 0.09,
    speedTokensPerSecond: 302.6,
    latencyFirstChunkSeconds: 0.4,
  },
  {
    id: 'gpt-5-codex-high',
    name: 'GPT-5 Codex (high)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: `GPT-5 Codex (high) là mô hình AI thế hệ mới của OpenAI, ra mắt tháng 4 năm 2025, được tối ưu hóa đặc biệt cho các tác vụ lập trình. Với khả năng hiểu ngữ cảnh code phức tạp và tuân thủ hướng dẫn chặt chẽ, GPT-5 Codex (high) mang lại hiệu suất vượt trội, giúp các nhà phát triển tăng tốc độ làm việc và xây dựng các ứng dụng phức tạp một cách hiệu quả hơn.`,
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    userRating: 4.5,
    ratingCount: 260,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 1 triệu token→ Tức là có thể xử lý khoảng 1,5 triệu từ hoặc 5.000 trang văn bản trong 1 lần xử lý.",
        "Chỉ số thông minh 53 →Dựa trên trung bình các điểm benmark là",
        "• MMLU-Pro (Reasoning & Knowledge) 81%",
        "• GPQA Diamond (Scientific Reasoning) 67%",
        "• Humanity's Exam (Reasoning & Knowledge) 4.6%",
        "• LiveCodeBench (Coding) 46%",
        "• SciCode (Coding) 38%",
        "• HumanEval (Coding) 96%",
        "• MATH-500 (Quantitative reasoning) 91%",
        "• AIME 2024 (Competition Math) 44%",
        "Giá trung bình 3.5 USD / 1 triệu token → Dựa trên các thông số giá",
        "• Giá đầu vào 2 USD / 1 triệu token.",
        "• Giá đầu ra 8 USD / 1 triệu token.",
        "Tốc độ sinh token là 97.2 token/s.",
        "Độ trễ 0.54s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '400k',
    intelligenceScore: 68,
    pricePerMillionTokens: 3.44,
    speedTokensPerSecond: 171.6,
    latencyFirstChunkSeconds: 40.71,
  },
  {
    id: 'deepseek-r1-jan25',
    name: 'DeepSeek R1 0528',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Deepseek',
    description: `+ DeepSeek R1-0528 là phiên bản nâng cấp mới nhất DeepSeek, được công bố vào tháng 5 năm 2025 và cũng là mô hình mã nguồn mở nên người dùng có thể triển khai cục bộ trên chính máy chủ của các bạn.
+ Phiên bản này đánh dấu bước tiến quan trọng trong khả năng suy luận sâu sắc, hiệu suất xử lý và tối ưu tài nguyên, giúp DeepSeek R1 có thể cạnh tranh trực tiếp cả về chất lượng và giá cả với các mô hình của Google và Open Al.
+ Không giống các mô hình "open-weight" nhưng hạn chế giấy phép (như LLaMA), DeepSeek R1 cho phép sử dụng thương mại, tùy chỉnh và triển khai tự do, khiến nó trở thành “trụ cột" mới cho các startup, nhà nghiên cứu, và ứng dụng Al độc lập.
+ Đổi lại là tốc độ sinh token khá chậm 28.6 token/s cho nên người dùng sẽ chờ đợi hơi lâu nhưng đó không là gì so với tiềm năng nó mang lại.`,
    logoUrl: '/image/Logo Deepseek cho bảng xếp hạng.png',
    link: 'https://www.deepseek.com/',
    userRating: 4.9,
    ratingCount: 155,
    features: [
        "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 128 nghìn token → Tức là có thể xử lý khoảng 120 nghìn từ hoặc 400 trang văn bản trong 1 lần xử lý.",
        "Chỉ số thông minh 68 →Dựa trên trung bình các điểm benmark là",
        "• MMLU-Pro (Reasoning & Knowledge) 85%",
        "• GPQA Diamond (Scientific Reasoning) 81%",
        "• Humanity's Exam (Reasoning & Knowledge) 14.9%",
        "• LiveCodeBench (Coding) 77%",
        "• SciCode (Coding) 40%",
        "• HumanEval (Coding) 97%",
        "• MATH-500 (Quantitative reasoning) 98%",
        "• AIME 2024 (Competition Math) 89%",
        "Giá trung bình 0.96 USD / 1 triệu token → Dựa trên các thông số giá",
        "• Giá đầu vào 0.55 USD / 1 triệu token.",
        "• Giá đầu ra 2.19 USD / 1 triệu token.",
        "Tốc độ sinh token là 28.6 token /s.",
        "Độ trễ 2.49s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '128k',
    intelligenceScore: 59,
    pricePerMillionTokens: 0.96,
    speedTokensPerSecond: 21.1,
    latencyFirstChunkSeconds: 3.32,
  },
  {
    id: 'openai-o3',
    name: 'Open AI o3',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'OpenAI',
    description: `+ OpenAI o3 là mô hình ngôn ngữ thế hệ mới do OpenAI phát triển, được ra mắt chính thức vào cuối năm 2024 như một bước tiến vượt bậc so với phiên bản tiền nhiệm o1. Đây là một mô hình transformer phản chiếu (reflective generative pre-trained transformer) được thiết kế đặc biệt để xử lý các câu hỏi đòi hỏi suy luận logic nhiều bước và tư duy phân tích sâu sắc.\n+ OpenAI o3 là một trong những mô hình đầu tiên của OpenAI có khả năng sử dụng công cụ một cách tự động trong quy trình suy luận, từ đó có thể truy cập thông tin thời gian thực, phân tích dữ liệu phức tạp và phối hợp nhiều khả năng để giải quyết các vấn đề đa bước hiệu quả hơn.\n+ OpenAI o3 đã áp dụng các kỹ thuật điều chỉnh và kiểm soát mới nhằm giảm thiểu các kết quả gây hại hoặc thiên lệch, đồng thời nâng cao tính minh bạch trong quá trình suy luận của mô hình.\n+ Mô hình cũng được tích hợp trong các dịch vụ như ChatGPT, API, Playground.`,
    logoUrl: '/image/Logo Open AI cho bảng xếp hạng.png',
    link: 'https://openai.com/',
    userRating: 4.9,
    ratingCount: 350,
    features: [],
    isFavorite: false,
    contextLengthToken: '200k',
    intelligenceScore: 67,
    pricePerMillionTokens: 3.5,
    speedTokensPerSecond: 229.0,
    latencyFirstChunkSeconds: 13.36,
  },
  {
    id: 'llama-nemotron-ultra-reasoning',
    name: 'Llama Nemotron Ultra Reasoning',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Nvidia',
    description: `+ Llama Nemotron Ultra là phiên bản cao cấp nhất trong dòng mô hình Llama Nemotron do NVIDIA phát triển, dựa trên nền tảng Llama 3.1 của Meta, với quy mô 253 tỷ tham số. Ra mắt đầu năm 2025, Nemotron Ultra được thiết kế đặc biệt để xử lý các tác vụ suy luận phức tạp, toán học nâng cao, lập trình và các nhiệm vụ khoa học với độ chính xác và hiệu suất vượt trội.
+ Mô hình được tối ưu đặc biệt cho hạ tầng GPU NVIDIA đa thiết bị, sử dụng các kỹ thuật như Neural Architecture Search (NAS) và Feed-Forward Network (FFN) Fusion giúp giảm đáng kể độ trễ và tăng thông lượng, đồng thời tiết kiệm bộ nhớ và chi phí vận hành trong môi trường trung tâm dữ liệu. Vì vậy cực kì thích hợp cho những cá nhân, công ty đang chạy phần cứng của Nvidia.
+ NVIDIA phát hành Llama Nemotron Ultra dưới giấy phép mở, cho phép doanh nghiệp và nhà phát triển sử dụng, tùy chỉnh và triển khai mô hình trong các ứng dụng thương mại với độ tin cậy cao.`,
    logoUrl: '/image/Logo Llama Nemotron Ultra Reasoning cho bảng xếp hạng.png',
    link: '#',
    userRating: 4.9,
    ratingCount: 188,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 128 nghìn token → Tức là có thể xử lý khoảng 120 nghìn từ hoặc 400 trang văn bản trong 1 lần xử lý.",
      "Chỉ số thông minh 61 →Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 83%",
      "• GPQA Diamond (Scientific Reasoning) 73%",
      "• Humanity's Exam (Reasoning & Knowledge) 8.1%",
      "• LiveCodeBench (Coding) 64%",
      "• SciCode (Coding) 35%",
      "• HumanEval (Coding)",
      "• MATH-500 (Quantitative reasoning) 95%",
      "• AIME 2024 (Competition Math) 75%",
      "Giá trung bình 0.9 USD / 1 triệu token → Dựa trên các thông số giá",
      "• Giá đầu vào 0.6 USD / 1 triệu token.",
      "• Giá đầu ra 1.8 USD / 1 triệu token.",
      "Tốc độ sinh token là 42.7 token /s.",
      "Độ trễ 0.66s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    contextLengthToken: '128k',
    intelligenceScore: 46,
    pricePerMillionTokens: 0.9,
    speedTokensPerSecond: 41.0,
    latencyFirstChunkSeconds: 0.67,
  },
  {
    id: 'deepseek-v3.1-reasoning',
    name: 'DeepSeek V3.1 (Reasoning)',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Deepseek',
    description: `+ DeepSeek V3.1 (Reasoning) là phiên bản kế nhiệm trong dòng mô hình DeepSeek, được tối ưu hóa đặc biệt cho các tác vụ đòi hỏi khả năng suy luận phức tạp, ra mắt vào tháng 8 năm 2025. Đây là mô hình mã nguồn mở, cho phép người dùng tự do triển khai trên hạ tầng riêng.
+ Phiên bản này tập trung vào việc cải thiện khả năng suy luận logic và giải quyết vấn đề nhiều bước, giúp nó cạnh tranh hiệu quả với các mô hình hàng đầu từ Google và OpenAI.
+ Giống như các phiên bản trước, DeepSeek V3.1 cho phép sử dụng thương mại và tùy chỉnh, trở thành một lựa chọn hấp dẫn cho các doanh nghiệp và nhà phát triển muốn xây dựng các ứng dụng AI độc lập.
+ Mô hình này cân bằng giữa hiệu suất suy luận cao và tốc độ xử lý hợp lý, mang lại một giải pháp mạnh mẽ cho các ứng dụng chuyên sâu.`,
    logoUrl: '/image/Logo Deepseek cho bảng xếp hạng.png',
    link: 'https://www.deepseek.com/',
    userRating: 4.4,
    ratingCount: 177,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý 1 lần là 128k token.",
      "Chỉ số thông minh 60 → Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 85%",
      "• GPQA Diamond (Scientific Reasoning) 78%",
      "• Humanity's Exam (Reasoning & Knowledge) 13%",
      "• LiveCodeBench (Coding) 78%",
      "• SciCode (Coding) 39%",
      "• IFBench (Instruction Following) 42%",
      "• AIME 2025 (Competition Math) 90%",
      "• AA-LCR (Long Context Reasoning) 53%",
      "Giá trung bình 0.96 USD / 1 triệu token.",
      "Tốc độ sinh token là 19.4 token/s.",
      "Độ trễ 2.96s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '128k',
    intelligenceScore: 60,
    pricePerMillionTokens: 0.96,
    speedTokensPerSecond: 19.4,
    latencyFirstChunkSeconds: 2.96,
  },
  {
    id: 'llama-4-maverick',
    name: 'Llama 4 Maverick',
    type: 'Mô hình ngôn ngữ lớn',
    developer: 'Meta',
    description: `Llama 4 Maverick là một trong những phiên bản nổi bật nhất thuộc dòng Llama 4 do Meta phát triển, chính thức ra mắt vào tháng 4 năm 2025. Đây là mô hình Al đa phương thức (multimodal) đầu tiên của Meta sử dụng kiến trúc Mixture-of-Experts (MoE) với khả năng xử lý hiệu quả, mạnh mẽ và tiết kiệm tài nguyên. Llama 4 Maverick sở hữu 17 tỷ tham số hoạt động (active parameters) trong tổng số khoảng 400 tỷ tham số, được tổ chức thành 128 chuyên gia (experts). Mỗi token đầu vào được xử lý bởi một chuyên gia riêng biệt cùng với một chuyên gia chung, giúp mô hình chỉ kích hoạt một phần tham số trong quá trình suy luận, từ đó giảm chi phí tính toán và độ trễ khi triển khai. Mô hình có thể chạy hiệu quả trên một máy chủ NVIDIA H100 DGX duy nhất hoặc triển khai phân tán để tối ưu hiệu suất. Llama 4 Maverick được huấn luyện trên dữ liệu đa ngôn ngữ với hơn 200 ngôn ngữ, hỗ trợ tốt 12 ngôn ngữ chính bao gồm tiếng Việt, tiếng Anh, tiếng Pháp, tiếng Đức, tiếng Thái và nhiều ngôn ngữ khác. Mô hình phù hợp với các ứng dụng đa dạng như trợ lý ảo, phân tích hình ảnh, dịch thuật, lập trình, và các hệ thống Al đa nhiệm trong doanh nghiệp.`,
    logoUrl: '/image/Logo LLMA cho bảng xếp hạng.png',
    link: 'https://ai.meta.com/llama/',
    userRating: 4.4,
    ratingCount: 160,
    features: [
      "Độ dài ngữ cảnh (context window) có thể xử lý một lần là 1 triệu token→ Tức là có thể xử lý khoảng 1,5 triệu từ hoặc 5.000 trang văn bản trong 1 lần xử lý.",
      "Chỉ số thông minh 51 →Dựa trên trung bình các điểm benmark là",
      "• MMLU-Pro (Reasoning & Knowledge) 81%",
      "• GPQA Diamond (Scientific Reasoning) 67%",
      "• Humanity's Exam (Reasoning & Knowledge) 4.8%",
      "• LiveCodeBench (Coding) 40%",
      "• SciCode (Coding) 33%",
      "• HumanEval (Coding) 88%",
      "• MATH-500 (Quantitative reasoning) 89%",
      "• AIME 2024 (Competition Math) 39%",
      "Giá trung bình 0.39 USD / 1 triệu token → Dựa trên các thông số giá",
      "• Giá đầu vào 0.22 USD / 1 triệu token.",
      "• Giá đầu ra 0.85 USD / 1 triệu token.",
      "Tốc độ sinh token là 164.4 token /s.",
      "Độ trễ 0.33s là thời gian chờ trung bình từ khi bấm “enter” gửi thông tin đến khi mô hình trả lời token đầu tiên."
    ],
    isFavorite: false,
    contextLengthToken: '1m',
    intelligenceScore: 42,
    pricePerMillionTokens: 0.39,
    speedTokensPerSecond: 167.8,
    latencyFirstChunkSeconds: 0.32,
  },
];

    
